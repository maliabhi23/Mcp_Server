{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmm\n"
     ]
    }
   ],
   "source": [
    "print(\"mmm\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n"
     ]
    }
   ],
   "source": [
    "print(\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\COMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\COMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\COMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing and TF-IDF representation completed. Outputs saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"sentimentdataset.csv\")\n",
    "\n",
    "# Select relevant columns\n",
    "text_column = \"Text\"       # Text data\n",
    "label_column = \"Sentiment\" # Sentiment labels\n",
    "\n",
    "# 1. Text Cleaning\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Lowercasing\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_text\"] = df[text_column].apply(clean_text)\n",
    "\n",
    "# 2. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "df[\"lemmatized_text\"] = df[\"cleaned_text\"].apply(lemmatize_text)\n",
    "\n",
    "# 3. Stop Word Removal\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df[\"final_text\"] = df[\"lemmatized_text\"].apply(remove_stopwords)\n",
    "\n",
    "# 4. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label_encoded\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "# Save Label Encoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as file:\n",
    "    pickle.dump(label_encoder, file)\n",
    "\n",
    "# 5. TF-IDF Representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df[\"final_text\"])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Save TF-IDF Model and Data\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "\n",
    "df.to_csv(\"cleaned_data.csv\", index=False)\n",
    "tfidf_df.to_csv(\"tfidf_representation.csv\", index=False)\n",
    "\n",
    "print(\"Text processing and TF-IDF representation completed. Outputs saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Classes: [' Acceptance   ' ' Acceptance      ' ' Accomplishment ' ' Admiration '\n",
      " ' Admiration   ' ' Admiration    ' ' Adoration    ' ' Adrenaline     '\n",
      " ' Adventure ' ' Affection    ' ' Amazement ' ' Ambivalence '\n",
      " ' Ambivalence     ' ' Amusement    ' ' Amusement     ' ' Anger        '\n",
      " ' Anticipation ' ' Anticipation  ' ' Anxiety   ' ' Anxiety         '\n",
      " ' Appreciation  ' ' Apprehensive ' ' Arousal       ' ' ArtisticBurst '\n",
      " ' Awe ' ' Awe    ' ' Awe          ' ' Awe           ' ' Bad '\n",
      " ' Betrayal ' ' Betrayal      ' ' Bitter       ' ' Bitterness '\n",
      " ' Bittersweet ' ' Blessed       ' ' Boredom ' ' Boredom         '\n",
      " ' Breakthrough ' ' Calmness     ' ' Calmness      ' ' Captivation '\n",
      " ' Celebration ' ' Celestial Wonder ' ' Challenge ' ' Charm ' ' Colorful '\n",
      " ' Compassion' ' Compassion    ' ' Compassionate ' ' Confidence    '\n",
      " ' Confident ' ' Confusion ' ' Confusion    ' ' Confusion       '\n",
      " ' Connection ' ' Contemplation ' ' Contentment ' ' Contentment   '\n",
      " ' Coziness     ' ' Creative Inspiration ' ' Creativity ' ' Creativity   '\n",
      " ' Culinary Adventure ' ' CulinaryOdyssey ' ' Curiosity ' ' Curiosity  '\n",
      " ' Curiosity   ' ' Curiosity     ' ' Curiosity       ' ' Darkness     '\n",
      " ' Dazzle        ' ' Desolation ' ' Despair ' ' Despair   '\n",
      " ' Despair      ' ' Despair         ' ' Desperation ' ' Determination '\n",
      " ' Determination   ' ' Devastated ' ' Disappointed ' ' Disappointment '\n",
      " ' Disgust ' ' Disgust      ' ' Disgust         ' ' Dismissive '\n",
      " ' DreamChaser   ' ' Ecstasy ' ' Elation   ' ' Elation       '\n",
      " ' Elegance ' ' Embarrassed ' ' Emotion ' ' EmotionalStorm '\n",
      " ' Empathetic ' ' Empowerment   ' ' Enchantment ' ' Enchantment   '\n",
      " ' Energy ' ' Engagement ' ' Enjoyment    ' ' Enthusiasm '\n",
      " ' Enthusiasm    ' ' Envious ' ' Envisioning History ' ' Envy            '\n",
      " ' Euphoria ' ' Euphoria   ' ' Euphoria     ' ' Euphoria      '\n",
      " ' Excitement ' ' Excitement   ' ' Excitement    ' ' Exhaustion '\n",
      " ' Exploration ' ' Fear         ' ' Fearful ' ' FestiveJoy    '\n",
      " ' Free-spirited ' ' Freedom       ' ' Friendship ' ' Frustrated '\n",
      " ' Frustration ' ' Frustration     ' ' Fulfillment  ' ' Fulfillment   '\n",
      " ' Grandeur ' ' Grateful ' ' Gratitude ' ' Gratitude  ' ' Gratitude   '\n",
      " ' Gratitude    ' ' Gratitude     ' ' Grief ' ' Grief      '\n",
      " ' Grief           ' ' Happiness ' ' Happiness    ' ' Happiness     '\n",
      " ' Happy ' ' Harmony ' ' Harmony    ' ' Harmony       ' ' Hate '\n",
      " ' Heartache ' ' Heartbreak ' ' Heartbreak    ' ' Heartwarming '\n",
      " ' Helplessness ' ' Helplessness    ' ' Hope ' ' Hope          '\n",
      " ' Hopeful ' ' Hypnotic ' ' Iconic ' ' Imagination ' ' Immersion '\n",
      " ' Indifference ' ' Indifference    ' ' InnerJourney  ' ' Inspiration '\n",
      " ' Inspiration  ' ' Inspiration   ' ' Inspired ' ' Intimidation '\n",
      " ' Intimidation    ' ' Intrigue      ' ' Isolation ' ' Jealous '\n",
      " ' Jealousy    ' ' Jealousy        ' ' Journey ' ' Joy ' ' Joy          '\n",
      " ' Joy in Baking ' ' JoyfulReunion ' ' Kind         ' ' Kindness '\n",
      " ' Loneliness ' ' Loneliness    ' ' Loneliness      ' ' Loss '\n",
      " ' LostLove ' ' Love ' ' Love         ' ' Marvel       ' ' Melancholy '\n",
      " ' Melancholy      ' ' Melodic       ' ' Mesmerizing ' ' Mindfulness   '\n",
      " ' Miscalculation ' ' Mischievous ' ' Motivation    ' \" Nature's Beauty \"\n",
      " ' Negative  ' ' Neutral ' ' Neutral   ' ' Nostalgia ' ' Nostalgia     '\n",
      " ' Nostalgia      ' ' Nostalgia       ' ' Numbness ' ' Numbness        '\n",
      " ' Obstacle ' \" Ocean's Freedom \" ' Optimism      ' ' Overjoyed     '\n",
      " ' Overwhelmed ' ' Overwhelmed   ' ' Pensive ' ' Playful '\n",
      " ' PlayfulJoy    ' ' Positive ' ' Positive  ' ' Positivity ' ' Pressure '\n",
      " ' Pride ' ' Pride        ' ' Pride         ' ' Proud ' ' Radiance    '\n",
      " ' Radiance      ' ' Reflection ' ' Reflection    ' ' Regret '\n",
      " ' Regret        ' ' Regret         ' ' Rejuvenation ' ' Relief '\n",
      " ' Renewed Effort ' ' Resentment      ' ' Resilience ' ' Resilience   '\n",
      " ' Reverence ' ' Reverence     ' ' Romance ' ' Ruins      '\n",
      " ' Runway Creativity ' ' Sad ' ' Sadness      ' ' Satisfaction '\n",
      " ' Satisfaction  ' ' Serenity ' ' Serenity   ' ' Serenity      '\n",
      " ' Serenity        ' ' Shame ' ' Shame        ' ' Solace ' ' Solitude '\n",
      " ' Sorrow ' ' Sorrow      ' ' Spark        ' ' Success ' ' Suffering '\n",
      " ' Surprise ' ' Surprise     ' ' Surprise      ' ' Suspense ' ' Sympathy '\n",
      " ' Tenderness    ' ' Thrill ' ' Thrill      ' ' Thrill        '\n",
      " ' Thrilling Journey ' ' Touched ' ' Tranquility ' ' Triumph '\n",
      " ' Vibrancy ' ' Whimsy        ' ' Whispers of the Past ' ' Winter Magic '\n",
      " ' Wonder ' ' Wonder     ' ' Wonder       ' ' Wonderment    ' ' Yearning '\n",
      " ' Zest ']\n",
      "Encoded Values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load Label Encoder\n",
    "with open(\"label_encoder.pkl\", \"rb\") as file:\n",
    "    label_encoder = pickle.load(file)\n",
    "\n",
    "# Check Label Mappings\n",
    "print(\"Label Classes:\", label_encoder.classes_)  # Shows original sentiment labels\n",
    "print(\"Encoded Values:\", list(range(len(label_encoder.classes_))))  # Shows corresponding encoded values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary (Top 10): ['enjoying', 'beautiful', 'day', 'park', 'traffic', 'wa', 'terrible', 'morning', 'finished', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load TF-IDF Vectorizer\n",
    "with open(\"tfidf_vectorizer.pkl\", \"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "\n",
    "# Check Vocabulary (Top 10 Features)\n",
    "print(\"TF-IDF Vocabulary (Top 10):\", list(tfidf_vectorizer.vocabulary_.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example new text\n",
    "new_text = [\"I love sunny days at the park!\"]\n",
    "\n",
    "# Transform text into TF-IDF representation\n",
    "new_text_tfidf = tfidf_vectorizer.transform(new_text)\n",
    "\n",
    "# Convert to array for viewing\n",
    "print(\"TF-IDF Representation:\\n\", new_text_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
