{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Objective: Take a document as input and Tokenize it, the remove its stop words and finally lemmatize it</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Document Loading</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The meaning of NLP is Natural Language Processing (NLP) which is a fascinating and rapidly evolving field that intersects computer science, artificial intelligence, and linguistics. NLP focuses on the interaction between computers and human language, enabling machines to understand, interpret, and generate human language in a way that is both meaningful and useful. With the increasing volume of text data generated every day, from social media posts to research articles, NLP has become an essential tool for extracting valuable insights and automating various tasks.\n"
     ]
    }
   ],
   "source": [
    "file=open(\"sample.txt\", mode=\"r\")\n",
    "\n",
    "text=file.readline()\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(\"Original Text: \" + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'meaning', 'of', 'NLP', 'is', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'which', 'is', 'a', 'fascinating', 'and', 'rapidly', 'evolving', 'field', 'that', 'intersects', 'computer', 'science', ',', 'artificial', 'intelligence', ',', 'and', 'linguistics', '.', 'NLP', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', ',', 'enabling', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'useful', '.', 'With', 'the', 'increasing', 'volume', 'of', 'text', 'data', 'generated', 'every', 'day', ',', 'from', 'social', 'media', 'posts', 'to', 'research', 'articles', ',', 'NLP', 'has', 'become', 'an', 'essential', 'tool', 'for', 'extracting', 'valuable', 'insights', 'and', 'automating', 'various', 'tasks', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stop Words Removal</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meaning', 'NLP', 'Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'rapidly', 'evolving', 'field', 'intersects', 'computer', 'science', 'artificial', 'intelligence', 'linguistics', 'NLP', 'focuses', 'interaction', 'computers', 'human', 'language', 'enabling', 'machines', 'understand', 'interpret', 'generate', 'human', 'language', 'way', 'meaningful', 'useful', 'increasing', 'volume', 'text', 'data', 'generated', 'every', 'day', 'social', 'media', 'posts', 'research', 'articles', 'NLP', 'become', 'essential', 'tool', 'extracting', 'valuable', 'insights', 'automating', 'various', 'tasks']\n"
     ]
    }
   ],
   "source": [
    "stop=set(stopwords.words(\"English\"))\n",
    "stop_word_list = list(stop)+list(punctuation)\n",
    "\n",
    "tokens_without_stop_words = [word for word in tokens if word.lower() not in stop_word_list]\n",
    "\n",
    "print(tokens_without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                LEMMA               \n",
      "\n",
      "meaning             mean                \n",
      "NLP                 NLP                 \n",
      "Natural             Natural             \n",
      "Language            Language            \n",
      "Processing          Processing          \n",
      "NLP                 NLP                 \n",
      "fascinating         fascinate           \n",
      "rapidly             rapidly             \n",
      "evolving            evolve              \n",
      "field               field               \n",
      "intersects          intersect           \n",
      "computer            computer            \n",
      "science             science             \n",
      "artificial          artificial          \n",
      "intelligence        intelligence        \n",
      "linguistics         linguistics         \n",
      "NLP                 NLP                 \n",
      "focuses             focus               \n",
      "interaction         interaction         \n",
      "computers           computers           \n",
      "human               human               \n",
      "language            language            \n",
      "enabling            enable              \n",
      "machines            machine             \n",
      "understand          understand          \n",
      "interpret           interpret           \n",
      "generate            generate            \n",
      "human               human               \n",
      "language            language            \n",
      "way                 way                 \n",
      "meaningful          meaningful          \n",
      "useful              useful              \n",
      "increasing          increase            \n",
      "volume              volume              \n",
      "text                text                \n",
      "data                data                \n",
      "generated           generate            \n",
      "every               every               \n",
      "day                 day                 \n",
      "social              social              \n",
      "media               media               \n",
      "posts               post                \n",
      "research            research            \n",
      "articles            article             \n",
      "NLP                 NLP                 \n",
      "become              become              \n",
      "essential           essential           \n",
      "tool                tool                \n",
      "extracting          extract             \n",
      "valuable            valuable            \n",
      "insights            insights            \n",
      "automating          automate            \n",
      "various             various             \n",
      "tasks               task                \n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = []\n",
    "print(\"{0:20}{1:20}\\n\".format(\"WORD\", \"LEMMA\"))\n",
    "for word in tokens_without_stop_words:\n",
    "    lemma = wordnet_lemmatizer.lemmatize(word, pos=\"v\")\n",
    "    lemmatized_words.append(lemma)\n",
    "    print(\"{0:20}{1:20}\".format(word, lemma))\n",
    "\n",
    "processed_text=\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean NLP Natural Language Processing NLP fascinate rapidly evolve field intersect computer science artificial intelligence linguistics NLP focus interaction computers human language enable machine understand interpret generate human language way meaningful useful increase volume text data generate every day social media post research article NLP become essential tool extract valuable insights automate various task'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Label Encoding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoded Output: [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform([processed_text])\n",
    "\n",
    "print(\"Label Encoded Output:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TF-IDF</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[0.11470787 0.11470787 0.11470787 0.11470787 0.11470787 0.11470787\n",
      "  0.11470787 0.11470787 0.11470787 0.11470787 0.11470787 0.11470787\n",
      "  0.11470787 0.11470787 0.11470787 0.11470787 0.22941573 0.22941573\n",
      "  0.11470787 0.11470787 0.11470787 0.11470787 0.11470787 0.11470787\n",
      "  0.3441236  0.11470787 0.11470787 0.11470787 0.11470787 0.11470787\n",
      "  0.11470787 0.45883147 0.11470787 0.11470787 0.11470787 0.11470787\n",
      "  0.11470787 0.11470787 0.11470787 0.11470787 0.11470787 0.11470787\n",
      "  0.11470787 0.11470787 0.11470787 0.11470787 0.11470787]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([processed_text]).toarray()\n",
    "\n",
    "np.save(\"tfidf.npy\", tfidf_matrix)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
